{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizing Numbers in MNIST Dataset\n",
    "\n",
    "Today's exercise is focused on recognition of hand written digits from the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Network\n",
    "\n",
    "For detection adn recongnition, we'll use a [LeNet model](https://en.wikipedia.org/wiki/LeNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Convolution (In LeNet-5, 32x32 images are given as input. Hence padding of 2 is done below)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "        # Max-pooling\n",
    "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        # Convolution\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n",
    "        # Max-pooling\n",
    "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = torch.nn.Linear(16*5*5, 120)   # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
    "        self.fc2 = torch.nn.Linear(120, 84)       # convert matrix with 120 features to a matrix of 84 features (columns)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)        # convert matrix with 84 features to a matrix of 10 features (columns)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.conv1(x))  \n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_1(x)\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_2(x)\n",
    "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\n",
    "        # read through https://stackoverflow.com/a/42482819/7551231\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        # FC-1, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        # FC-2, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        # FC-3\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "We train the model. You can use saved loss in a text file for charting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model):\n",
    "    model.train()\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    num_epochs = 5\n",
    "    p = 1\n",
    "    with open(\"loss.txt\", \"wt\") as f:\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, sample in enumerate(data, 0):\n",
    "                optimizer.zero_grad()            \n",
    "                #print(sample[0])\n",
    "                #print(sample[1])\n",
    "                inputs = sample[0]\n",
    "                #img = np.reshape(inputs, (1, 1, 28, 28)) / 255\n",
    "                #img = torch.from_numpy(img)\n",
    "                #img = img.type(torch.FloatTensor)\n",
    "                labels = sample[1]\n",
    "                \n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                if i % 500 == 499:    # print every 500 mini-batches\n",
    "                    print('[%d, %d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 500))\n",
    "                    s = \"{0} {1}\\n\".format(p, running_loss / 500)\n",
    "                    f.write(s)\n",
    "                    p += 1\n",
    "                    running_loss = 0.0\n",
    "\n",
    "    torch.save({'state_dict': model.state_dict()}, './model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation\n",
    "\n",
    "We validate our trained model to the validation set.\n",
    "\n",
    "You can enable/disable displaying of each validated image by changing the value of `show_image` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(data, model):\n",
    "    model.eval()\n",
    "    print(\"Validating...\")\n",
    "    show_image = True  \n",
    "\n",
    "    size = len(data)\n",
    "    num_incorrect = 0\n",
    "    i = 0\n",
    "    for sample in data:\n",
    "        images, labels = sample\n",
    "        #img = transforms.functional.to_pil_image(images[0][0], mode='L')\n",
    "        #img.save(\"img_{}.png\".format(i), \"png\")\n",
    "        output = model(images)\n",
    "        predicted = torch.max(output.data, 1)\n",
    "        if labels[0] != predicted[1].item():\n",
    "            num_incorrect += 1\n",
    "            if show_image: \n",
    "                s = \"Real: {0}\\t Predicted: {1}\".format(labels[0], predicted[1].item())\n",
    "                print(s)\n",
    "                imshow(torchvision.utils.make_grid(images))\n",
    "        i += 1\n",
    "    print(\"Validation Error: {0} %\".format(100.0 * num_incorrect / size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Task\n",
    "\n",
    "Implement a sliding window to recognize numbers in any location in a given image. We do not expect numbers to be rotated, so this is much simplified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(model, image, size):\n",
    "    \"\"\"\n",
    "    Implement a sliding window to recognize numbers in any location in a given image.\n",
    "    We do not expect numbers to be rotated, so this is much simplified.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Whole Thing\n",
    "\n",
    "On the first run, `DataLoader` will download MNIST dataset using `torchvision`'s class.\n",
    "\n",
    "Also, one trained you don't have to train the model on the next run, just uncoment the `torch.load` line and comment `model = LeNet()` and `train` function.\n",
    "\n",
    "Uncoment `sliding_window` to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    transform = torchvision.transforms.Compose([torchvision.transforms.Grayscale(), torchvision.transforms.Resize(28), torchvision.transforms.ToTensor()])\n",
    "    \n",
    "    batch_size_train = 16\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform), batch_size=batch_size_train, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('./data', train=False, download=True, transform=transform))\n",
    "\n",
    "    #trainfolder = datasets.ImageFolder(\"train\", transform)\n",
    "    #train_loader = torch.utils.data.DataLoader(trainfolder, batch_size=batch_size_train, shuffle=True)\n",
    "    \n",
    "    model = LeNet() \n",
    "    #model = torch.load(\"./model.pth\") \n",
    "\n",
    "    train(train_loader, model)\n",
    "    validation(test_loader, model)\n",
    "\n",
    "    #img = cv2.imread('numbers.png', 0)\n",
    "    #sliding_window(model, img, (28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 500] loss: 2.293\n",
      "[1, 1000] loss: 1.781\n",
      "[1, 1500] loss: 0.620\n",
      "[1, 2000] loss: 0.401\n",
      "[1, 2500] loss: 0.320\n",
      "[1, 3000] loss: 0.244\n",
      "[1, 3500] loss: 0.202\n",
      "[2, 500] loss: 0.160\n",
      "[2, 1000] loss: 0.150\n",
      "[2, 1500] loss: 0.129\n",
      "[2, 2000] loss: 0.127\n",
      "[2, 2500] loss: 0.119\n",
      "[2, 3000] loss: 0.099\n",
      "[2, 3500] loss: 0.107\n",
      "[3, 500] loss: 0.090\n",
      "[3, 1000] loss: 0.092\n",
      "[3, 1500] loss: 0.086\n",
      "[3, 2000] loss: 0.084\n",
      "[3, 2500] loss: 0.081\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
