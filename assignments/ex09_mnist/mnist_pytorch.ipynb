{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizing Numbers in MNIST Dataset\n",
    "\n",
    "Today's exercise is focused on recognition of hand written digits from the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database).\n",
    "\n",
    "You also need a [Pillow](https://pillow.readthedocs.io/en/stable/) package in your Python virtual env. Install it by running: `pip install Pillow`.\n",
    "\n",
    "A brief introduction to MNIST and own model is described in [PyTorch 1.2 Quickstart with Google Colab](https://medium.com/dair-ai/pytorch-1-2-quickstart-with-google-colab-6690a30c38d) or [A Gentle Introduction to PyTorch 1.2](https://medium.com/dair-ai/pytorch-1-2-introduction-guide-f6fa9bb7597c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow# as pyplot_imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Network\n",
    "\n",
    "For detection adn recongnition, we'll use a [LeNet model](https://en.wikipedia.org/wiki/LeNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Convolution (In LeNet-5, 32x32 images are given as input. Hence padding of 2 is done below)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "        # Max-pooling\n",
    "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        # Convolution\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n",
    "        # Max-pooling\n",
    "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = torch.nn.Linear(16*5*5, 120)   # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
    "        self.fc2 = torch.nn.Linear(120, 84)       # convert matrix with 120 features to a matrix of 84 features (columns)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)        # convert matrix with 84 features to a matrix of 10 features (columns)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_1(x)\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_2(x)\n",
    "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\n",
    "        # read through https://stackoverflow.com/a/42482819/7551231\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        # FC-1, then perform ReLU non-linearity\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # FC-2, then perform ReLU non-linearity\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # FC-3\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "We train the model. You can use saved loss in a text file for charting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model):\n",
    "    model.train()\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    num_epochs = 5\n",
    "    p = 1\n",
    "    with open(\"loss.txt\", \"wt\") as f:\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, sample in enumerate(data, 0):\n",
    "                optimizer.zero_grad()            \n",
    "                #print(sample[0])\n",
    "                #print(sample[1])\n",
    "                inputs = sample[0]\n",
    "                #img = np.reshape(inputs, (1, 1, 28, 28)) / 255\n",
    "                #img = torch.from_numpy(img)\n",
    "                #img = img.type(torch.FloatTensor)\n",
    "                labels = sample[1]\n",
    "                \n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                if i % 500 == 499:    # print every 500 mini-batches\n",
    "                    print('[%d, %d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 500))\n",
    "                    s = \"{0} {1}\\n\".format(p, running_loss / 500)\n",
    "                    f.write(s)\n",
    "                    p += 1\n",
    "                    running_loss = 0.0\n",
    "\n",
    "    torch.save(model.state_dict(), './model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation\n",
    "\n",
    "We validate our trained model to the validation set.\n",
    "\n",
    "You can enable/disable displaying of each wrongly recognized image by changing the value of `show_image` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(data, model):\n",
    "    model.eval()\n",
    "    print(\"Validating...\")\n",
    "    show_image = False\n",
    "\n",
    "    size = len(data)\n",
    "    num_incorrect = 0\n",
    "    i = 0\n",
    "    for sample in data:\n",
    "        images, labels = sample\n",
    "        #img = transforms.functional.to_pil_image(images[0][0], mode='L')\n",
    "        #img.save(\"img_{}.png\".format(i), \"png\")\n",
    "        output = model(images)\n",
    "        predicted = torch.max(output.data, 1)\n",
    "        if labels[0] != predicted[1].item():\n",
    "            num_incorrect += 1\n",
    "            if show_image: \n",
    "                s = \"Real: {0}\\t Predicted: {1}\".format(labels[0], predicted[1].item())\n",
    "                print(s)\n",
    "                my_imshow(torchvision.utils.make_grid(images))\n",
    "        i += 1\n",
    "    print(\"Validation Error: {0} %\".format(100.0 * num_incorrect / size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Task\n",
    "\n",
    "Implement a sliding window to recognize numbers in any location in a given image. We do not expect numbers to be rotated, so this is much simplified.\n",
    "\n",
    "You're expected to draw a rectangle around each detected number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe26f73d358>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we use an RGB version of image, so it can be displayed.\n",
    "# In your expiriment, use 'numbers.png'\n",
    "numbers_img = Image.open('numbers_rgb.png')\n",
    "\n",
    "imshow(np.asarray(numbers_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(model, image, size):\n",
    "    \"\"\"\n",
    "    Implement a sliding window to recognize numbers in any location in a given image.\n",
    "    We do not expect numbers to be rotated, so this is much simplified.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Whole Thing\n",
    "\n",
    "On the first run, `DataLoader` will download MNIST dataset using `torchvision`'s class.\n",
    "\n",
    "Also, one trained you don't have to train the model on the next run, just uncoment the `torch.load_state_dict` line and comment `model = LeNet()` and `train` function.\n",
    "\n",
    "Uncoment `sliding_window` to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    transform = torchvision.transforms.Compose([torchvision.transforms.Grayscale(), torchvision.transforms.Resize(28), torchvision.transforms.ToTensor()])\n",
    "    \n",
    "    batch_size_train = 16\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform), batch_size=batch_size_train, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('./data', train=False, download=True, transform=transform))\n",
    "\n",
    "    #trainfolder = datasets.ImageFolder(\"train\", transform)\n",
    "    #train_loader = torch.utils.data.DataLoader(trainfolder, batch_size=batch_size_train, shuffle=True)\n",
    "    \n",
    "    # create instance of a model\n",
    "    model = LeNet()\n",
    "    \n",
    "    # train new model\n",
    "    train(train_loader, model)\n",
    "    \n",
    "    # use existing model\n",
    "    #model.load_state_dict(torch.load('./model.pth'))\n",
    "    \n",
    "    validation(test_loader, model)\n",
    "\n",
    "    # uncoment to run sliding window\n",
    "    #img = Image.load('numbers.png')\n",
    "    #sliding_window(model, img, (28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 500] loss: 2.295\n",
      "[1, 1000] loss: 1.951\n",
      "[1, 1500] loss: 0.676\n",
      "[1, 2000] loss: 0.415\n",
      "[1, 2500] loss: 0.307\n",
      "[1, 3000] loss: 0.232\n",
      "[1, 3500] loss: 0.219\n",
      "[2, 500] loss: 0.158\n",
      "[2, 1000] loss: 0.150\n",
      "[2, 1500] loss: 0.145\n",
      "[2, 2000] loss: 0.122\n",
      "[2, 2500] loss: 0.125\n",
      "[2, 3000] loss: 0.119\n",
      "[2, 3500] loss: 0.116\n",
      "[3, 500] loss: 0.094\n",
      "[3, 1000] loss: 0.097\n",
      "[3, 1500] loss: 0.094\n",
      "[3, 2000] loss: 0.090\n",
      "[3, 2500] loss: 0.086\n",
      "[3, 3000] loss: 0.087\n",
      "[3, 3500] loss: 0.085\n",
      "[4, 500] loss: 0.079\n",
      "[4, 1000] loss: 0.072\n",
      "[4, 1500] loss: 0.075\n",
      "[4, 2000] loss: 0.083\n",
      "[4, 2500] loss: 0.070\n",
      "[4, 3000] loss: 0.061\n",
      "[4, 3500] loss: 0.067\n",
      "[5, 500] loss: 0.052\n",
      "[5, 1000] loss: 0.063\n",
      "[5, 1500] loss: 0.058\n",
      "[5, 2000] loss: 0.063\n",
      "[5, 2500] loss: 0.069\n",
      "[5, 3000] loss: 0.050\n",
      "[5, 3500] loss: 0.061\n",
      "Validating...\n",
      "Validation Error: 1.8 %\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
